{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c0fae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3a312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark started successfully\n",
      "Version: 3.5.5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Kafka-Keyspaces-Write\")\n",
    "    .master(\"local[*]\")\n",
    "    \n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.5,\"\n",
    "        \"org.postgresql:postgresql:42.7.8\"\n",
    "    )\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "print(\"Spark started successfully\")\n",
    "print(\"Version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abefa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b6e1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType , DoubleType\n",
    "\n",
    "fund_schema = StructType([\n",
    "    StructField(\"fund_id\", IntegerType(), True),\n",
    "    StructField(\"fund_name\", StringType(), True),\n",
    "    StructField(\"fund_code\", StringType(), True),\n",
    "    StructField(\"fund_description\", StringType(), True),\n",
    "    StructField(\"updated_at\", LongType(), True),\n",
    "    StructField(\"fund_price\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "source_schema = StructType([\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"connector\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"ts_ms\", LongType(), True),\n",
    "    StructField(\"snapshot\", StringType(), True),\n",
    "    StructField(\"db\", StringType(), True),\n",
    "    StructField(\"sequence\", StringType(), True),\n",
    "    StructField(\"schema\", StringType(), True),\n",
    "    StructField(\"table\", StringType(), True),\n",
    "    StructField(\"txId\", LongType(), True),\n",
    "    StructField(\"lsn\", LongType(), True),\n",
    "    StructField(\"xmin\", StringType(), True)\n",
    "])\n",
    "\n",
    "cdc_schema = StructType([\n",
    "    StructField(\"before\", fund_schema, True),\n",
    "    StructField(\"after\", fund_schema, True),\n",
    "    StructField(\"source\", source_schema, True),\n",
    "    StructField(\"op\", StringType(), True),\n",
    "    StructField(\"ts_ms\", LongType(), True),\n",
    "    StructField(\"transaction\", StringType(), True),\n",
    "    StructField(\"source_system\", StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e090e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\")\n",
    "    .option(\"subscribe\", \"pgsrc.public.fund_metadata\")\n",
    "    .option(\"startingOffsets\", \"earliest\")  # all existing + new messages\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0fec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "parsed_df = df.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), cdc_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, to_timestamp\n",
    "\n",
    "actual_df = (\n",
    "    parsed_df.select(\n",
    "        col(\"op\").alias(\"Operation\"),\n",
    "        to_timestamp((col(\"ts_ms\") / 1000)).alias(\"ProcessTime\"),\n",
    "        col(\"before\"),\n",
    "        col(\"after\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"Values\",\n",
    "        when(col(\"Operation\") == \"d\", col(\"before\"))  \n",
    "        .otherwise(col(\"after\"))                      \n",
    "    )\n",
    "    .drop(\"before\", \"after\")\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "final_df = (\n",
    "    actual_df\n",
    "    .filter(\n",
    "        col(\"Operation\").isNotNull() &\n",
    "        col(\"Values.fund_id\").isNotNull()\n",
    "    )\n",
    "    .select(\n",
    "        col(\"Operation\"),\n",
    "        col(\"ProcessTime\"),\n",
    "        col(\"Values.fund_id\").alias(\"fund_id\"),\n",
    "        col(\"Values.fund_name\").alias(\"fund_name\"),\n",
    "        col(\"Values.fund_code\").alias(\"fund_code\"),\n",
    "        col(\"Values.fund_description\").alias(\"fund_description\"),\n",
    "        to_timestamp(col(\"Values.updated_at\") / 1_000_000).alias(\"updated_at\"),\n",
    "        col(\"Values.fund_price\").alias(\"fund_price\")\n",
    "    ).withColumn(\n",
    "        \"delete_flag\",\n",
    "        when(col(\"Operation\") == \"d\", \"Y\").otherwise(\"N\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"effective_date\",\n",
    "        current_timestamp()\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9400afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbc_url = \"jdbc:postgresql://localhost:5432/finance\"\n",
    "jdbc_properties = {\n",
    "    \"user\": \"pguser\",\n",
    "    \"password\": \"pgpassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "def write_to_multiple_sink(batch_df, batch_id):\n",
    "\n",
    "    print(f\"\\n--- Writing Batch {batch_id} ---\")\n",
    "    batch_df.show(truncate=False) \n",
    "    print(\"COLUMNS:\", batch_df.columns)\n",
    "\n",
    "    try:\n",
    "        batch_df.write \\\n",
    "            .mode(\"append\") \\\n",
    "            .jdbc(url=jdbc_url, table=\"public.fund_metadata_trail\", properties=jdbc_properties)\n",
    "        \n",
    "        print(f\"Batch {batch_id} written to Postgres.\")\n",
    "    except Exception as e:\n",
    "        print (\"[FAIL] ERROR while writing batch in Postgres:\", batch_id)\n",
    "        print(e)\n",
    "        raise e\n",
    "\n",
    "    # try:\n",
    "    #     batch_df.write \\\n",
    "    #         .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    #         .options(\n",
    "    #             table=\"fund_metadata_trail\",\n",
    "    #             keyspace=\"fund_metadata\"\n",
    "    #         ) \\\n",
    "    #         .mode(\"append\") \\\n",
    "    #         .save()\n",
    "        \n",
    "    #     print(f\"Batch {batch_id} written to AWS Keyspaces.\")\n",
    "    # except Exception as e:\n",
    "    #     print (\"[FAIL] ERROR while writing batch in Cassandra:\", batch_id)\n",
    "    #     print(e)\n",
    "    #     raise e        \n",
    "\n",
    "query = final_df.writeStream.foreachBatch(write_to_multiple_sink).start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ecfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.isActive  # True if running, False if stopped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea78e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a815de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
