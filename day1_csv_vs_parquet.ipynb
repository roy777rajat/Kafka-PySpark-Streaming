{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d787ee6",
   "metadata": {},
   "source": [
    "### First Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4bb4f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI URL: http://DESKTOP-KRET721:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Day1-Simple Hands-on\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print(\"Spark UI URL:\", spark.sparkContext.uiWebUrl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a7bbd417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 541909\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"E:\\pyspark-training\\data\\small\\online_retail.csv\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "\n",
    "print(\"Row count:\", df.count())\n",
    "df.show(5)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e310ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the count\n",
    "df.count()\n",
    "# to know the first 10 line\n",
    "df.show(10)\n",
    "# Print Specific column\n",
    "df.select (\"InvoiceNo\",\"StockCode\").show()\n",
    "# Rename columns\n",
    "df.select (\"InvoiceNo\",\"StockCode\",\"Quantity\").withColumnRenamed(\"InvoiceNo\",\"InvoiceNumber\").withColumnRenamed(\"Quantity\",\"Count\").show(5)\n",
    "# Multiple Column Renamed style\n",
    "df.select (\"InvoiceNo\",\"StockCode\",\"Quantity\").withColumnsRenamed({\"InvoiceNo\":\"InvoiceNumber\"}).show(3)\n",
    "# Case like , length, cast in one go\n",
    "from pyspark.sql.functions import col, when, length\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df.select(\"InvoiceNo\", \"StockCode\", \"Quantity\" , \"Description\",\"CustomerID\") \\\n",
    "    .withColumn(\"Number\", \n",
    "        when((col(\"Quantity\") > 5) & (col(\"Quantity\") <= 7), 'Large') \\\n",
    "        .when(col(\"Quantity\") > 7, \"Very Large\") \\\n",
    "        .when(col(\"Quantity\") < 5, \"Less\") \\\n",
    "        .otherwise(\"NA\")) \\\n",
    "    .withColumn(\"LengthDescription\", length(col(\"Description\"))) \\\n",
    "    .withColumn(\"CustomerID\", col(\"CustomerID\").cast(IntegerType())) \\\n",
    "    .withColumnsRenamed({\"InvoiceNo\": \"InvoiceNumber\"}) \\\n",
    "    .show(15)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd8eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND and OR Condition\n",
    "df.select(col(\"*\")).filter((col(\"InvoiceNo\") == \"536365\") & (col(\"Quantity\") == 6)).show()\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df.select\\\n",
    "    (\n",
    "       col(\"InvoiceNo\"),\\\n",
    "       col(\"StockCode\"),\\\n",
    "       col(\"Description\"),\\\n",
    "       col(\"UnitPrice\").alias(\"CHAHA\")\\\n",
    "       \n",
    "    )\\\n",
    "    .withColumn(\"MyNewColumn\",lit(\"KAKA\"))\\\n",
    "    .filter\\\n",
    "    (\\\n",
    "        col(\"InvoiceNo\") == \"536365\"\\\n",
    "    ).show(20)\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7f2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, avg, min, max\n",
    "\n",
    "# -- Group BY / Aggregation\n",
    "df.groupBy(col(\"Country\"),col(\"CustomerID\")).agg(\n",
    "    sum(\"Quantity\").alias(\"Total_Quantity\"),\n",
    "    avg(\"UnitPrice\").alias(\"Average_UnitPrice\"),\n",
    "    min(\"UnitPrice\").alias(\"Min_UnitPrice\"),\n",
    "    max(\"UnitPrice\").alias(\"Max_UnitPrice\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fcf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -- Please share distinct country count and then list\n",
    "df.select(col(\"Country\"),col(\"CustomerID\")).distinct().show()\n",
    "df.select(col(\"Country\")).distinct().count()\n",
    "\n",
    "\n",
    "# -- Creating data frame from count and Union / UnionALL\n",
    "distinct_country_cust_count_dataframe = df.select(col(\"Country\"),col(\"CustomerID\")).distinct().count()\n",
    "distinct_country_cust_count_dataframe = spark.createDataFrame([('Dual', distinct_country_cust_count_dataframe)], ['Metric', 'Count'])\n",
    "\n",
    "distinct_country_count_dataframe = df.select(col(\"Country\")).distinct().count()\n",
    "distinct_country_count_dataframe = spark.createDataFrame([('Country', distinct_country_count_dataframe)], ['Metric', 'Count'])\n",
    "\n",
    "final_df = distinct_country_cust_count_dataframe.unionAll(distinct_country_count_dataframe)\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628a2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hands on Task 1 :Count total rows, distinct invoices, distinct customers\n",
    "# Count Total Row\n",
    "df.count()\n",
    "# Distinct Invoive\n",
    "df.select(col(\"InvoiceNo\")).distinct().show()\n",
    "# Distinct Customer\n",
    "df.select(col(\"CustomerID\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 : Filter high-value orders (TotalPrice > 100000)\n",
    "from pyspark.sql.functions import desc;\n",
    "df.groupBy(col(\"CustomerID\"))\\\n",
    "    .agg(sum(col(\"UnitPrice\") * col(\"Quantity\")).alias(\"TotalPrice\"))\\\n",
    "    .filter((col(\"TotalPrice\") > \"100000\") & (col(\"CustomerID\").isNotNull() ))\\\n",
    "    .sort(col(\"TotalPrice\").desc())\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcfa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 : Group the data by Country and compute total quantity sold and average unit price per country.\n",
    "from pyspark.sql.functions import sum,avg,col\n",
    "df.groupBy(col(\"Country\")).agg(\n",
    "    sum(col(\"Quantity\")).alias(\"CountryWiseQuantityCount\"), \n",
    "    avg(col(\"UnitPrice\")).alias(\"CountryWiseAvgUnitPrice\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc001e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: -Add a new column TotalPrice = UnitPrice * Quantity to the DataFrame.\n",
    "df.select(col(\"*\")).withColumn(\"TotalPrice\",(col(\"UnitPrice\") * col(\"Quantity\"))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af64d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 5 : Create a new column HighValue using a when condition: HighValue = \"YES\" if TotalPrice > 20 Otherwise HighValue = \"NO\"\n",
    "\n",
    "df.select(col(\"*\"))\\\n",
    "    .withColumn(\"HighValue\",when((col(\"UnitPrice\") * col(\"Quantity\")) > \"20\" , \"YES\").otherwise(\"NO\"))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1172ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6 : Sort the DataFrame by Quantity in descending order and show top 10 rows\n",
    "df.select(col(\"*\"))\\\n",
    "    .sort(col(\"Quantity\").desc())\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c5976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7 : Convert InvoiceDate to timestamp and extract:\n",
    "from pyspark.sql.functions import to_timestamp,dayofmonth,month,year\n",
    "df.withColumn(\\\n",
    "    \"InvoiceDateTS\",\\\n",
    "    to_timestamp(col(\"InvoiceDate\")))\\\n",
    "    .withColumn(\"DAY_OF_MONTH\",dayofmonth(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "    .withColumn(\"MONTH\",month(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "    .withColumn(\"YEAR\",year(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "        .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7272552f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+------------------+------------------+\n",
      "|       Country|HighValue|      TotalRevenue|      AvgUnitPrice|\n",
      "+--------------+---------+------------------+------------------+\n",
      "|United Kingdom|      YES| 5086314.069999987|2.9566691711599886|\n",
      "|United Kingdom|       NO|1381472.0400000298|1.2539407911132832|\n",
      "|   Netherlands|      YES| 278939.5899999998|2.1741276595744683|\n",
      "|          EIRE|      YES|186201.80999999994| 3.172618006993011|\n",
      "|     Australia|      YES|132982.75999999998| 2.575443349753696|\n",
      "|       Germany|      YES|118887.29000000027|3.2335112847222267|\n",
      "|        France|      YES|105018.14000000001|2.8237074057246745|\n",
      "|       Germany|       NO| 64332.64000000005|1.3796503496503552|\n",
      "|        France|       NO| 55515.54000000013|1.3886212745335613|\n",
      "|          EIRE|       NO| 47580.98000000002|1.3661680092059876|\n",
      "|         Japan|      YES|36984.840000000004|2.2230705394190875|\n",
      "|   Switzerland|      YES|35760.659999999996|3.1687823439878238|\n",
      "|        Sweden|      YES|          34210.24|1.9216460905349795|\n",
      "|         Spain|      YES|32935.549999999996| 3.955821917808219|\n",
      "|        Norway|      YES|23424.330000000005| 3.371182033096926|\n",
      "|       Denmark|      YES|16266.770000000004|3.4814356435643563|\n",
      "|       Belgium|       NO|14763.849999999986|1.4194439295644101|\n",
      "|      Portugal|      YES|14357.269999999997| 2.610781671159029|\n",
      "|         Spain|       NO|14341.649999999987|1.4161357210179062|\n",
      "|       Belgium|      YES|14339.590000000004|2.9566417910447753|\n",
      "+--------------+---------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['TotalRevenue DESC NULLS LAST], true\n",
      "+- Aggregate [Country#6358, HighValue#6432], [Country#6358, HighValue#6432, sum(round(TotalPrice#6422, 2)) AS TotalRevenue#6496, avg(UnitPrice#6356) AS AvgUnitPrice#6498]\n",
      "   +- Filter (Quantity#6354 > 5)\n",
      "      +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, DAY#6443, MONTH#6455, year(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS YEAR#6468]\n",
      "         +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, DAY#6443, month(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS MONTH#6455]\n",
      "            +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, dayofmonth(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS DAY#6443]\n",
      "               +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, CASE WHEN (TotalPrice#6422 > cast(20 as double)) THEN YES ELSE NO END AS HighValue#6432]\n",
      "                  +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, (UnitPrice#6356 * cast(Quantity#6354 as double)) AS TotalPrice#6422]\n",
      "                     +- Relation [InvoiceNo#6351,StockCode#6352,Description#6353,Quantity#6354,InvoiceDate#6355,UnitPrice#6356,CustomerID#6357,Country#6358] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Country: string, HighValue: string, TotalRevenue: double, AvgUnitPrice: double\n",
      "Sort [TotalRevenue#6496 DESC NULLS LAST], true\n",
      "+- Aggregate [Country#6358, HighValue#6432], [Country#6358, HighValue#6432, sum(round(TotalPrice#6422, 2)) AS TotalRevenue#6496, avg(UnitPrice#6356) AS AvgUnitPrice#6498]\n",
      "   +- Filter (Quantity#6354 > 5)\n",
      "      +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, DAY#6443, MONTH#6455, year(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS YEAR#6468]\n",
      "         +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, DAY#6443, month(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS MONTH#6455]\n",
      "            +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, HighValue#6432, dayofmonth(cast(to_timestamp(InvoiceDate#6355, None, TimestampType, Some(Europe/London), false) as date)) AS DAY#6443]\n",
      "               +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, TotalPrice#6422, CASE WHEN (TotalPrice#6422 > cast(20 as double)) THEN YES ELSE NO END AS HighValue#6432]\n",
      "                  +- Project [InvoiceNo#6351, StockCode#6352, Description#6353, Quantity#6354, InvoiceDate#6355, UnitPrice#6356, CustomerID#6357, Country#6358, (UnitPrice#6356 * cast(Quantity#6354 as double)) AS TotalPrice#6422]\n",
      "                     +- Relation [InvoiceNo#6351,StockCode#6352,Description#6353,Quantity#6354,InvoiceDate#6355,UnitPrice#6356,CustomerID#6357,Country#6358] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [TotalRevenue#6496 DESC NULLS LAST], true\n",
      "+- Aggregate [Country#6358, HighValue#6432], [Country#6358, HighValue#6432, sum(round(TotalPrice#6422, 2)) AS TotalRevenue#6496, avg(UnitPrice#6356) AS AvgUnitPrice#6498]\n",
      "   +- Project [UnitPrice#6356, Country#6358, TotalPrice#6422, CASE WHEN (TotalPrice#6422 > 20.0) THEN YES ELSE NO END AS HighValue#6432]\n",
      "      +- Project [UnitPrice#6356, Country#6358, (UnitPrice#6356 * cast(Quantity#6354 as double)) AS TotalPrice#6422]\n",
      "         +- Filter (isnotnull(Quantity#6354) AND (Quantity#6354 > 5))\n",
      "            +- Relation [InvoiceNo#6351,StockCode#6352,Description#6353,Quantity#6354,InvoiceDate#6355,UnitPrice#6356,CustomerID#6357,Country#6358] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [TotalRevenue#6496 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(TotalRevenue#6496 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=6285]\n",
      "      +- HashAggregate(keys=[Country#6358, HighValue#6432], functions=[sum(round(TotalPrice#6422, 2)), avg(UnitPrice#6356)], output=[Country#6358, HighValue#6432, TotalRevenue#6496, AvgUnitPrice#6498])\n",
      "         +- Exchange hashpartitioning(Country#6358, HighValue#6432, 200), ENSURE_REQUIREMENTS, [plan_id=6282]\n",
      "            +- HashAggregate(keys=[Country#6358, HighValue#6432], functions=[partial_sum(round(TotalPrice#6422, 2)), partial_avg(UnitPrice#6356)], output=[Country#6358, HighValue#6432, sum#6518, sum#6519, count#6520L])\n",
      "               +- Project [UnitPrice#6356, Country#6358, TotalPrice#6422, CASE WHEN (TotalPrice#6422 > 20.0) THEN YES ELSE NO END AS HighValue#6432]\n",
      "                  +- Project [UnitPrice#6356, Country#6358, (UnitPrice#6356 * cast(Quantity#6354 as double)) AS TotalPrice#6422]\n",
      "                     +- Filter (isnotnull(Quantity#6354) AND (Quantity#6354 > 5))\n",
      "                        +- FileScan csv [Quantity#6354,UnitPrice#6356,Country#6358] Batched: false, DataFilters: [isnotnull(Quantity#6354), (Quantity#6354 > 5)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/E:/pyspark-training/data/small/online_retail.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Quantity), GreaterThan(Quantity,5)], ReadSchema: struct<Quantity:int,UnitPrice:double,Country:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Task 8: Write a single PySpark query that does the following:\n",
    "\n",
    "# 1. Add a column TotalPrice = UnitPrice * Quantity\n",
    "# 2. Add a column HighValue = \"YES\" if TotalPrice > 20, else \"NO\"\n",
    "# 3. Convert InvoiceDate to timestamp and extract Year, Month, Day\n",
    "# 5. Filter only rows where Quantity > 5\n",
    "# 6. Group by Country and HighValue flag:\n",
    "# 7. Compute sum(TotalPrice) as TotalRevenue\n",
    "# 8. Compute avg(UnitPrice) as AvgUnitPrice\n",
    "# 9. Sort the final result by TotalRevenue descending\n",
    "\n",
    "from pyspark.sql.functions import round\n",
    "final_df = df.withColumn(\"TotalPrice\", (col(\"UnitPrice\") * col(\"Quantity\")))\\\n",
    "    .withColumn(\"HighValue\" , when(col(\"TotalPrice\") > 20 , \"YES\").otherwise(\"NO\"))\\\n",
    "    .withColumn(\"DAY\" , dayofmonth(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "    .withColumn(\"MONTH\" , month(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "    .withColumn(\"YEAR\" , year(to_timestamp(col(\"InvoiceDate\"))))\\\n",
    "    .filter(col(\"Quantity\") > 5)\\\n",
    "    .groupBy(col(\"Country\"), col(\"HighValue\")).agg(\\\n",
    "        sum(round(col(\"TotalPrice\"),2)).alias(\"TotalRevenue\")\\\n",
    "       ,avg(col(\"UnitPrice\")).alias(\"AvgUnitPrice\")\\\n",
    "    )\\\n",
    "    .sort(col(\"TotalRevenue\").desc())\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "final_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b69b803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8d04e51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.submitTime', '1766661581283'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.driver.port', '49650'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'DESKTOP-KRET721.mshome.net'),\n",
       " ('spark.sql.warehouse.dir', 'file:/E:/pyspark-training/spark-warehouse'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.id', 'local-1766684149847'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.startTime', '1766684149638'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'Day1-Simple Hands-on')]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "807529e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.defaultParallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "107df550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.rdd.getNumPartitions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
